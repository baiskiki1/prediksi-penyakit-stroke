# -*- coding: utf-8 -*-
"""apkstroke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T5vlieABtBQqk4VNhIJ5NzXDOKyapjsE
"""

import streamlit as st
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import joblib # For saving/loading the scaler

# --- 1. Load Model and Scaler (pre-trained) ---
# Assuming the model is saved as 'stroke_prediction_model.h5'
# Assuming the scaler for numerical features is saved as 'scaler.joblib'

@st.cache_resource
def load_artifacts():
    model = load_model('stroke_prediction_model.h5')
    # In a real scenario, you would save and load the scaler
    # For this example, we'll recreate a dummy scaler based on the training data's structure
    # In a deployed app, ensure your scaler is saved from training and loaded here.

    # Re-create a dummy scaler to match the preprocessing steps
    # Get numerical columns used during training
    numerical_cols_for_scaler = ['age', 'avg_glucose_level', 'bmi']

    # Create a dummy scaler. In a real application, you'd load the fitted scaler.
    # For demonstration, we'll use mean/std from X_train if X_train was passed here,
    # but generally, the scaler itself should be saved/loaded.

    # Let's assume we have access to X_train statistics for demonstration purposes.
    # In a real deployment, you'd save the fitted scaler.
    # For now, we'll use placeholder means and stds as we don't have X_train directly here.
    # A robust solution involves saving the actual fitted StandardScaler.

    # Placeholder: if you have X_train from the notebook, you'd do:
    # scaler = StandardScaler()
    # scaler.fit(X_train[numerical_cols])
    # joblib.dump(scaler, 'scaler.joblib')
    # then here: scaler = joblib.load('scaler.joblib')

    # Since we can't directly use X_train here, we'll create a scaler instance
    # It's critical that the scaler used here is the *same* one fitted on the training data.
    # This part is a simplification for a colab environment, a deployed app needs to save the scaler.
    scaler = StandardScaler()
    # We need to simulate the fit_transform on X_train to have the same mean/std
    # Since we cannot access X_train, we will skip loading/saving for now and
    # rely on the fact that numerical columns will be scaled later IF we have the original data.
    # For a fully functional Streamlit app, make sure to save and load your scaler.
    try:
        scaler = joblib.load('scaler.joblib') # Attempt to load if previously saved
    except FileNotFoundError:
        st.warning("Scaler not found. Please ensure 'scaler.joblib' is saved from training. Using a new scaler for demonstration, which might not yield accurate results if not fitted on original training data.")
        # Fallback if scaler isn't saved: create a fresh one and inform the user.
        # This is not ideal for deployment but necessary for a standalone script without the original fit.
        # For this specific notebook context, we'll assume a dummy fit or that scaling is handled externally.
        # A better approach would be to include the scaler fit/save in the notebook and load it here.
        # Given the previous steps, `scaler` was fitted. We need to persist it.
        pass # We will handle scaling in the preprocessing function directly if scaler is not loaded


    return model, scaler

model, scaler = load_artifacts()

# --- 2. Streamlit App Interface ---
st.title('Stroke Prediction App')
st.write('Enter patient details to predict the likelihood of stroke.')

# --- User Inputs ---
with st.sidebar:
    st.header('Patient Data Input')
    gender = st.selectbox('Gender', ['Male', 'Female', 'Other'])
    age = st.slider('Age', 0, 100, 40)
    hypertension = st.selectbox('Hypertension', ['No', 'Yes'])
    heart_disease = st.selectbox('Heart Disease', ['No', 'Yes'])
    ever_married = st.selectbox('Ever Married', ['No', 'Yes'])
    work_type = st.selectbox('Work Type', ['Private', 'Self-employed', 'Govt_job', 'children', 'Never_worked'])
    residence_type = st.selectbox('Residence Type', ['Urban', 'Rural'])
    avg_glucose_level = st.slider('Average Glucose Level', 50.0, 300.0, 100.0)
    bmi = st.slider('BMI', 10.0, 60.0, 25.0)
    smoking_status = st.selectbox('Smoking Status', ['formerly smoked', 'never smoked', 'smokes', 'Unknown'])

# --- Preprocessing User Input ---
def preprocess_input(gender, age, hypertension, heart_disease, ever_married,
                       work_type, residence_type, avg_glucose_level, bmi, smoking_status, scaler):

    # Create a dictionary for the input features
    input_data = {
        'gender': gender,
        'age': age,
        'hypertension': 1 if hypertension == 'Yes' else 0,
        'heart_disease': 1 if heart_disease == 'Yes' else 0,
        'ever_married': ever_married,
        'work_type': work_type,
        'Residence_type': residence_type,
        'avg_glucose_level': avg_glucose_level,
        'bmi': bmi,
        'smoking_status': smoking_status
    }

    # Convert to DataFrame
    df_input = pd.DataFrame([input_data])

    # Apply the same preprocessing steps as training
    # 1. Handle 'Other' gender (if it exists, though Streamlit selectbox prevents it)
    #    The training code removed 'Other' entries, so we should ensure consistency or remove it.
    #    Here we assume the Streamlit app user will not select 'Other' due to the selectbox options.

    # 2. Drop 'id' column (not present in input_data)

    # 3. Encode categorical features using one-hot encoding
    categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

    # Create dummy columns for all possible categories to ensure consistent feature order
    # This is crucial because get_dummies might produce different columns if a category is missing.
    gender_options = ['Male', 'Female'] # 'Other' is removed
    ever_married_options = ['Yes', 'No']
    work_type_options = ['Private', 'Self-employed', 'Govt_job', 'children', 'Never_worked']
    residence_type_options = ['Urban', 'Rural']
    smoking_status_options = ['formerly smoked', 'never smoked', 'smokes', 'Unknown']

    # Set up dummy columns with all possible values for consistency
    for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:
        df_input[col] = pd.Categorical(df_input[col], categories=eval(f"{col.lower()}_options"))

    df_processed = pd.get_dummies(df_input, columns=categorical_cols, drop_first=True)

    # Ensure all expected columns from training are present, fill missing with 0
    # Get the list of columns X_train had after preprocessing (excluding 'stroke')
    # This list should be saved and loaded with the scaler in a real app.
    # For this notebook context, we'll list them out based on the X_train.columns from the kernel state.
    expected_columns = [
        'age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi',
        'gender_Male', 'ever_married_Yes', 'work_type_Never_worked', 'work_type_Private',
        'work_type_Self-employed', 'work_type_children', 'Residence_type_Urban',
        'smoking_status_formerly smoked', 'smoking_status_never smoked', 'smoking_status_smokes'
    ]

    # Add missing columns with 0
    for col in expected_columns:
        if col not in df_processed.columns:
            df_processed[col] = 0

    # Reorder columns to match the training data's feature order
    df_processed = df_processed[expected_columns]

    # 4. Scale numerical features (age, avg_glucose_level, bmi)
    numerical_cols = ['age', 'avg_glucose_level', 'bmi']
    if scaler is not None and hasattr(scaler, 'mean_'): # Check if scaler was actually loaded/fitted
        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])
    else: # Fallback if scaler wasn't loaded properly - this will scale based on current input.
          # NOT IDEAL for production, but for demonstration where scaler wasn't saved.
          # A proper solution involves saving and loading the *fitted* scaler.
        temp_scaler = StandardScaler()
        # Fit and transform is not correct for single sample prediction, must use pre-fitted scaler
        # For this context, we will not scale if the pre-fitted scaler is not available.
        # This highlights the importance of saving the fitted scaler.
        st.warning("Numerical features were NOT scaled as the fitted scaler was not available. Prediction might be inaccurate.")
        # If you were to force scale with a new scaler (not recommended for single prediction):
        # df_processed[numerical_cols] = temp_scaler.fit_transform(df_processed[numerical_cols])

    return df_processed

# --- Prediction Button and Logic ---
if st.button('Predict Stroke Risk'):
    processed_input = preprocess_input(gender, age, hypertension, heart_disease, ever_married,
                                       work_type, residence_type, avg_glucose_level, bmi, smoking_status, scaler)

    prediction_proba = model.predict(processed_input)[0][0]
    prediction = (prediction_proba > 0.5).astype(int)

    st.subheader('Prediction Result:')
    if prediction == 1:
        st.error(f'High Risk of Stroke! (Probability: {prediction_proba:.2f})')
    else:
        st.success(f'Low Risk of Stroke. (Probability: {prediction_proba:.2f})')

    st.write('---')
    st.subheader('Input Features:')
    st.write(processed_input)

# Important Note:
# In a real-world scenario, you MUST save the StandardScaler object (e.g., using joblib)
# during your training phase, and then load it here in your Streamlit app.
# The current setup for the scaler is a workaround for a Colab environment where
# the scaler object from the training notebook might not be directly available
# when running this script as a standalone Streamlit app.